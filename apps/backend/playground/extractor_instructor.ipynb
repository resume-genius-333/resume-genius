{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c796a00",
   "metadata": {},
   "source": [
    "# Resume Extractor with Instructor\n",
    "\n",
    "This notebook demonstrates extracting structured data from PDF resumes using the Instructor library with LiteLLM proxy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d1f4b5",
   "metadata": {},
   "source": [
    "## Setup Path\n",
    "\n",
    "Add the backend directory to Python path for proper imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e26e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the backend directory to the Python path\n",
    "backend_dir = Path.cwd().parent\n",
    "sys.path.insert(0, str(backend_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proxy-config",
   "metadata": {},
   "source": [
    "## Configure Proxy Settings\n",
    "\n",
    "Ensure localhost connections bypass proxy settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proxy-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['NO_PROXY'] = 'localhost,127.0.0.1'\n",
    "os.environ['no_proxy'] = 'localhost,127.0.0.1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6506587",
   "metadata": {},
   "source": [
    "## Import and Setup Instructor with LiteLLM\n",
    "\n",
    "Configure Instructor to work with your LiteLLM proxy using the OpenAI client wrapper.\n",
    "\n",
    "For the `.env` file, use the template in `.env.example` under the `backend` directory.\n",
    "\n",
    "- `LITELLM_BASE_URL`: Should be `http://127.0.0.1:4000` (use 127.0.0.1, not 0.0.0.0)\n",
    "- `LITELLM_API_KEY`: Virtual key from LiteLLM UI at http://127.0.0.1:4000/ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0579b298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import instructor\n",
    "from openai import AsyncOpenAI\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Load .env from the parent (backend) directory\n",
    "env_path = Path.cwd().parent / '.env'\n",
    "load_dotenv(env_path)\n",
    "\n",
    "# Validate environment variables\n",
    "base_url = os.environ.get('LITELLM_BASE_URL')\n",
    "api_key = os.environ.get('LITELLM_API_KEY')\n",
    "\n",
    "if not base_url:\n",
    "    raise ValueError(\"LITELLM_BASE_URL not found in environment variables\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"LITELLM_API_KEY not found in environment variables\")\n",
    "\n",
    "# Add /v1 to the base URL for OpenAI API compatibility\n",
    "if not base_url.endswith('/v1'):\n",
    "    base_url = f\"{base_url}/v1\"\n",
    "\n",
    "print(f\"Connecting to LiteLLM at: {base_url}\")\n",
    "\n",
    "# Create Instructor client with LiteLLM proxy\n",
    "client = instructor.from_openai(\n",
    "    AsyncOpenAI(\n",
    "        base_url=base_url,\n",
    "        api_key=api_key,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "schemas",
   "metadata": {},
   "source": [
    "## Define Extraction Schemas\n",
    "\n",
    "Import the existing EducationLLMSchema and create a wrapper for multiple education entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "schema-definition",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.schemas.llm.education import EducationLLMSchema\n",
    "\n",
    "class EducationList(BaseModel):\n",
    "    \"\"\"Wrapper for extracting multiple education experiences from a resume.\"\"\"\n",
    "    \n",
    "    education_entries: List[EducationLLMSchema] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"List of all education experiences found in the resume\"\n",
    "    )\n",
    "    \n",
    "    extraction_notes: Optional[str] = Field(\n",
    "        None,\n",
    "        description=\"Any notes or observations about the extraction process\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-extraction",
   "metadata": {},
   "source": [
    "## Extract Single Education Entry\n",
    "\n",
    "Simple extraction of a single education experience using Instructor's clean API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extract-single",
   "metadata": {},
   "outputs": [],
   "source": [
    "from instructor.multimodal import PDF\n",
    "\n",
    "# Extract single education entry\n",
    "single_education = await client.chat.completions.create(\n",
    "    model='gpt-4o-mini',  # or any model available in your LiteLLM proxy\n",
    "    response_model=EducationLLMSchema,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                \"Extract the most recent education experience from this resume.\",\n",
    "                PDF.from_path(\"./files/sample1.pdf\")\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Display the extracted data\n",
    "print(\"Single Education Entry:\")\n",
    "print(single_education.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-extraction",
   "metadata": {},
   "source": [
    "## Extract Multiple Education Entries\n",
    "\n",
    "Extract all education experiences from the resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extract-multiple",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all education entries\n",
    "all_education = await client.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    response_model=EducationList,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                \"Extract ALL education experiences from this resume. Include degrees, certifications, and relevant training programs.\",\n",
    "                PDF.from_path(\"./files/sample1.pdf\")\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Display all extracted education entries\n",
    "print(f\"Found {len(all_education.education_entries)} education entries:\\n\")\n",
    "for i, edu in enumerate(all_education.education_entries, 1):\n",
    "    print(f\"Entry {i}:\")\n",
    "    print(f\"  Institution: {edu.institution_name}\")\n",
    "    print(f\"  Degree: {edu.degree}\")\n",
    "    print(f\"  Field: {edu.field_of_study}\")\n",
    "    print(f\"  Duration: {edu.start_date} - {edu.end_date}\")\n",
    "    if edu.gpa and edu.max_gpa:\n",
    "        print(f\"  GPA: {edu.gpa}/{edu.max_gpa}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "url-extraction",
   "metadata": {},
   "source": [
    "## Extract from URL\n",
    "\n",
    "Instructor also supports extracting from PDFs hosted at URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extract-url",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with URL (replace with actual URL)\n",
    "# pdf_url = \"https://example.com/resume.pdf\"\n",
    "# \n",
    "# education_from_url = await client.chat.completions.create(\n",
    "#     model='gpt-4o-mini',\n",
    "#     response_model=EducationLLMSchema,\n",
    "#     messages=[\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": [\n",
    "#                 \"Extract the education information from this resume.\",\n",
    "#                 PDF.from_url(pdf_url)\n",
    "#             ]\n",
    "#         }\n",
    "#     ]\n",
    "# )\n",
    "# \n",
    "# print(education_from_url.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "error-handling",
   "metadata": {},
   "source": [
    "## Error Handling and Retries\n",
    "\n",
    "Instructor provides built-in retry logic for failed extractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "with-retries",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "# Configure custom retry logic\n",
    "@retry(\n",
    "    stop=stop_after_attempt(3),\n",
    "    wait=wait_exponential(multiplier=1, min=4, max=10)\n",
    ")\n",
    "async def extract_with_retries(pdf_path: str) -> EducationList:\n",
    "    \"\"\"Extract education with automatic retries on failure.\"\"\"\n",
    "    return await client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        response_model=EducationList,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    \"Extract all education experiences from this resume.\",\n",
    "                    PDF.from_path(pdf_path)\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        max_retries=2  # Instructor's built-in retries\n",
    "    )\n",
    "\n",
    "# Use the function with retries\n",
    "# result = await extract_with_retries(\"./files/sample1.pdf\")\n",
    "# print(f\"Successfully extracted {len(result.education_entries)} entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completeness",
   "metadata": {},
   "source": [
    "## Check Extraction Completeness\n",
    "\n",
    "Use the completeness calculation from your base schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-completeness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check completeness of extracted data\n",
    "if single_education:\n",
    "    completeness = single_education.calculate_completeness()\n",
    "    print(f\"Data completeness: {completeness:.1f}%\")\n",
    "    \n",
    "    # Show which fields are missing\n",
    "    missing_fields = [\n",
    "        field_name for field_name in EducationLLMSchema.model_fields\n",
    "        if getattr(single_education, field_name, None) is None\n",
    "    ]\n",
    "    if missing_fields:\n",
    "        print(f\"Missing fields: {', '.join(missing_fields)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "batch-processing",
   "metadata": {},
   "source": [
    "## Batch Processing Multiple PDFs\n",
    "\n",
    "Process multiple resume PDFs efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "batch-process",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import Dict, List\n",
    "\n",
    "async def process_resume(pdf_path: Path) -> Dict:\n",
    "    \"\"\"Process a single resume and return results.\"\"\"\n",
    "    try:\n",
    "        result = await client.chat.completions.create(\n",
    "            model='gpt-4o-mini',\n",
    "            response_model=EducationList,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        \"Extract all education information from this resume.\",\n",
    "                        PDF.from_path(str(pdf_path))\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        return {\n",
    "            \"file\": pdf_path.name,\n",
    "            \"success\": True,\n",
    "            \"data\": result,\n",
    "            \"entry_count\": len(result.education_entries)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"file\": pdf_path.name,\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "# Process multiple PDFs concurrently\n",
    "async def batch_process_resumes(pdf_dir: Path) -> List[Dict]:\n",
    "    \"\"\"Process all PDF files in a directory.\"\"\"\n",
    "    pdf_files = list(pdf_dir.glob(\"*.pdf\"))\n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    # Process all PDFs concurrently\n",
    "    results = await asyncio.gather(\n",
    "        *[process_resume(pdf) for pdf in pdf_files]\n",
    "    )\n",
    "    \n",
    "    # Summary\n",
    "    successful = sum(1 for r in results if r[\"success\"])\n",
    "    print(f\"\\nProcessing complete: {successful}/{len(pdf_files)} successful\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage (uncomment to run)\n",
    "# results = await batch_process_resumes(Path(\"./files\"))\n",
    "# for result in results:\n",
    "#     if result[\"success\"]:\n",
    "#         print(f\"{result['file']}: {result['entry_count']} education entries\")\n",
    "#     else:\n",
    "#         print(f\"{result['file']}: Failed - {result['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advantages",
   "metadata": {},
   "source": [
    "## Key Advantages of Using Instructor\n",
    "\n",
    "1. **Simplified Code**: No manual file upload/deletion or complex parsing\n",
    "2. **Type Safety**: Direct Pydantic model returns with validation\n",
    "3. **Provider Agnostic**: Same code works with OpenAI, Anthropic, or any LiteLLM-supported model\n",
    "4. **Built-in Retries**: Automatic retry logic for failed extractions\n",
    "5. **Clean API**: Intuitive interface with `response_model` parameter\n",
    "6. **No File Management**: No need to track file IDs or clean up uploads\n",
    "7. **Flexible Input**: Support for local files, URLs, and base64 strings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}